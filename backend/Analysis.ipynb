{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7984e43-beaf-46b0-8649-1c039c0e4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve, StratifiedKFold\n",
    "\n",
    "def plot_learning_curve(model, X, y, title, scoring=\"roc_auc\"):\n",
    "    \"\"\"\n",
    "    Plots a learning curve (training vs. cross-val score) for any classifier.\n",
    "    \n",
    "    model    : an unfitted sklearn estimator (or one you want to re-fit)\n",
    "    X, y     : training data (we’ll re-fit on subsets)\n",
    "    title    : plot title\n",
    "    scoring  : metric, e.g. 'roc_auc', 'accuracy', 'neg_log_loss'\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    train_sizes = np.linspace(0.1, 1.0, 5)\n",
    "\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator   = model,\n",
    "        X           = X,\n",
    "        y           = y,\n",
    "        cv          = cv,\n",
    "        scoring     = scoring,\n",
    "        train_sizes = train_sizes,\n",
    "        n_jobs      = -1,\n",
    "        shuffle     = True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std  = np.std (train_scores, axis=1)\n",
    "    val_mean   = np.mean(val_scores,   axis=1)\n",
    "    val_std    = np.std (val_scores,   axis=1)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', label='Training score')\n",
    "    plt.fill_between(train_sizes,\n",
    "                     train_mean - train_std,\n",
    "                     train_mean + train_std,\n",
    "                     alpha=0.2)\n",
    "    plt.plot(train_sizes, val_mean, 'o-', label='Validation score')\n",
    "    plt.fill_between(train_sizes,\n",
    "                     val_mean - val_std,\n",
    "                     val_mean + val_std,\n",
    "                     alpha=0.2)\n",
    "    plt.xlabel('Number of training examples')\n",
    "    plt.ylabel(scoring)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b839a7-2dd9-4c86-87de-0a13017b7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost learning curve\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import classification_report, log_loss\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import learning_curve, StratifiedKFold\n",
    "\n",
    "# # 1) Re-instantiate your model (or use the one you already have)\n",
    "# xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# plot_learning_curve(\n",
    "#     model    = xgb,\n",
    "#     X        = X_resampled,\n",
    "#     y        = y_resampled,\n",
    "#     title    = \"Learning Curve — XGBoost\",\n",
    "#     scoring  = \"roc_auc\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d23d59-a4b1-4cd4-971a-50c938add534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost classification report + log loss\n",
    "\n",
    "xgb.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "proba_xgb = xgb.predict_proba(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"Log Loss:\", log_loss(y_test, proba_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbe544-d928-4f50-9bd6-f662ed8f113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt='d')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b409c25-c436-4b8a-a780-24034fef98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XGBoost test results (not very helpful in determining the quality of the model)\n",
    "\n",
    "# results_df = X_test.copy()\n",
    "# results_df['Actual'] = y_test.values\n",
    "# results_df['Predicted'] = y_pred_xgb\n",
    "# results_df['Champion_Prob'] = proba_xgb[:, 1]\n",
    "# results_df['Index'] = X_test_indices  # Save index for merging\n",
    "\n",
    "# # Get metadata for the test indices\n",
    "# test_metadata = playoff_teams.loc[X_test_indices, ['TEAM_NAME', 'SEASON']].copy()\n",
    "\n",
    "# # You can manually define which teams are East/West\n",
    "# east_teams = ['Boston Celtics', 'Milwaukee Bucks', 'Miami Heat', 'Philadelphia 76ers', 'Atlanta Hawks', 'Cleveland Cavaliers', 'Brooklyn Nets', 'Toronto Raptors', 'Chicago Bulls', 'Indiana Pacers', 'New York Knicks', 'Orlando Magic', 'Washington Wizards', 'Detroit Pistons', 'Charlotte Hornets']\n",
    "# test_metadata['CONFERENCE'] = test_metadata['TEAM_NAME'].apply(lambda x: 'East' if x in east_teams else 'West')\n",
    "\n",
    "# results_df = pd.DataFrame(X_test, index=X_test_indices)  # Match index\n",
    "# results_df['Champion_Prob'] = proba_xgb[:, 1]\n",
    "# results_df['Predicted'] = y_pred_xgb\n",
    "# results_df['Actual'] = y_test.values\n",
    "\n",
    "# # Merge with metadata\n",
    "# results_df = results_df.merge(test_metadata, left_index=True, right_index=True)\n",
    "\n",
    "# top_by_conference = (\n",
    "#     results_df\n",
    "#     .groupby(['SEASON', 'CONFERENCE'])\n",
    "#     .apply(lambda df: df.nlargest(1, 'Champion_Prob'))\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# top_by_conference = top_by_conference.sort_values(by='SEASON')\n",
    "\n",
    "# print(top_by_conference[['SEASON', 'CONFERENCE', 'TEAM_NAME', 'Champion_Prob', 'Actual']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3cedb-c5e6-4444-a7a6-64bb5866695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XGBoost feature importance\n",
    "\n",
    "# from xgboost import plot_importance\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plot_importance(xgb, max_num_features=20, importance_type='gain')\n",
    "# plt.title(\"XGBoost Feature Importance (Top 20)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768925d6-c718-45bb-b885-2726fa3c2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XGBoost ROC curve\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, proba_xgb[:, 1])\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # 4) Plot\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.plot(fpr, tpr, label=f\"XGBoost (AUC = {roc_auc:.3f})\")\n",
    "# plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Guess\")\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"ROC Curve — XGBoost Champion Model\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f4048-f793-4ff8-bbc1-edee5125ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LR learning curve\n",
    "\n",
    "# plot_learning_curve(\n",
    "#     model    = lr,\n",
    "#     X        = X_resampled,\n",
    "#     y        = y_resampled,\n",
    "#     title    = \"Learning Curve — Logistic Regression\",\n",
    "#     scoring  = \"roc_auc\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46123644-a8f2-4329-9f79-bb8c284bbc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LR classification report\n",
    "\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# y_pred_lr = lr.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc3383-62db-4920-baeb-d7131160c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LR confusion matrix\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d')\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db845e-678f-4544-809d-5c5c80467c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR log loss\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "proba_lr = lr.predict_proba(X_test)\n",
    "print(\"Log Loss:\", log_loss(y_test, proba_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067860f2-ab97-4d45-9d3f-39b72bacec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LR test results (not helpful)\n",
    "\n",
    "# results_df = X_test.copy()\n",
    "# results_df['Actual'] = y_test.values\n",
    "# results_df['Predicted'] = y_pred_lr\n",
    "# results_df['Champion_Prob'] = proba_lr[:, 1]\n",
    "# results_df['Index'] = X_test_indices  # Save index for merging\n",
    "\n",
    "# # Get metadata for the test indices\n",
    "# test_metadata = playoff_teams.loc[X_test_indices, ['TEAM_NAME', 'SEASON']].copy()\n",
    "\n",
    "# # You can manually define which teams are East/West\n",
    "# east_teams = ['Boston Celtics', 'Milwaukee Bucks', 'Miami Heat', 'Philadelphia 76ers', 'Atlanta Hawks', 'Cleveland Cavaliers', 'Brooklyn Nets', 'Toronto Raptors', 'Chicago Bulls', 'Indiana Pacers', 'New York Knicks', 'Orlando Magic', 'Washington Wizards', 'Detroit Pistons', 'Charlotte Hornets']\n",
    "# test_metadata['CONFERENCE'] = test_metadata['TEAM_NAME'].apply(lambda x: 'East' if x in east_teams else 'West')\n",
    "\n",
    "# results_df = pd.DataFrame(X_test, index=X_test_indices)  # Match index\n",
    "# results_df['Champion_Prob'] = proba_lr[:, 1]\n",
    "# results_df['Predicted'] = y_pred_lr\n",
    "# results_df['Actual'] = y_test.values\n",
    "\n",
    "# # Merge with metadata\n",
    "# results_df = results_df.merge(test_metadata, left_index=True, right_index=True)\n",
    "\n",
    "# top_by_conference = (\n",
    "#     results_df\n",
    "#     .groupby(['SEASON', 'CONFERENCE'])\n",
    "#     .apply(lambda df: df.nlargest(1, 'Champion_Prob'))\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# top_by_conference = top_by_conference.sort_values(by='SEASON')\n",
    "\n",
    "# print(top_by_conference[['SEASON', 'CONFERENCE', 'TEAM_NAME', 'Champion_Prob', 'Actual']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8349b-2721-467f-95d7-6c7a9107e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LR ROC curve\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, proba_lr[:, 1])\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # 4) Plot\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.plot(fpr, tpr, label=f\"LR (AUC = {roc_auc:.3f})\")\n",
    "# plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Guess\")\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"ROC Curve — LR Champion Model\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ffc1f0-baa0-476b-8554-d03b98270287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LR feature importance\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 1) Extract absolute coefficient values and pair with feature names\n",
    "# coef = lr.coef_[0]\n",
    "# imp = pd.Series(data=abs(coef), index=X_test.columns)\n",
    "\n",
    "# # 2) Sort descending and (optionally) take top 10\n",
    "# imp = imp.sort_values(ascending=False).head(10)\n",
    "\n",
    "# # 3) Plot bar chart\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# imp.plot.barh()\n",
    "# plt.gca().invert_yaxis()               # largest at top\n",
    "# plt.xlabel(\"Absolute Coefficient Value\")\n",
    "# plt.title(\"Top 10 Feature Importances — Logistic Regression\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2974ed0-cdda-4915-a115-ceef7a6811bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RF classification report\n",
    "\n",
    "# print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc4d17-63fe-4045-a678-ab14355d42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RF confusion matrix\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d')\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ff845-4bbb-4a0d-b0bd-2916348432b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF learning curve\n",
    "\n",
    "# plot_learning_curve(\n",
    "#     model    = rf,\n",
    "#     X        = X_resampled,\n",
    "#     y        = y_resampled,\n",
    "#     title    = \"Learning Curve — Random Forest\",\n",
    "#     scoring  = \"roc_auc\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3a3f2-99b1-41ff-be71-7c09fc1e8860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RF log loss\n",
    "\n",
    "# proba_rf = rf.predict_proba(X_test)\n",
    "\n",
    "# print(\"Log Loss:\", log_loss(y_test, proba_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d696c05-8ca3-4e2b-b8bf-06fd3dbbf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RF test results (not helpful)\n",
    "\n",
    "# results_df = X_test.copy()\n",
    "# results_df['Actual'] = y_test.values\n",
    "# results_df['Predicted'] = y_pred_rf\n",
    "# results_df['Champion_Prob'] = proba_rf[:, 1]\n",
    "# results_df['Index'] = X_test_indices  # Save index for merging\n",
    "\n",
    "# # Get metadata for the test indices\n",
    "# test_metadata = playoff_teams.loc[X_test_indices, ['TEAM_NAME', 'SEASON']].copy()\n",
    "\n",
    "# # You can manually define which teams are East/West\n",
    "# east_teams = ['Boston Celtics', 'Milwaukee Bucks', 'Miami Heat', 'Philadelphia 76ers', 'Atlanta Hawks', 'Cleveland Cavaliers', 'Brooklyn Nets', 'Toronto Raptors', 'Chicago Bulls', 'Indiana Pacers', 'New York Knicks', 'Orlando Magic', 'Washington Wizards', 'Detroit Pistons', 'Charlotte Hornets']\n",
    "# test_metadata['CONFERENCE'] = test_metadata['TEAM_NAME'].apply(lambda x: 'East' if x in east_teams else 'West')\n",
    "\n",
    "# results_df = pd.DataFrame(X_test, index=X_test_indices)  # Match index\n",
    "# results_df['Champion_Prob'] = proba_rf[:, 1]\n",
    "# results_df['Predicted'] = y_pred_rf\n",
    "# results_df['Actual'] = y_test.values\n",
    "\n",
    "# # Merge with metadata\n",
    "# results_df = results_df.merge(test_metadata, left_index=True, right_index=True)\n",
    "\n",
    "# top_by_conference = (\n",
    "#     results_df\n",
    "#     .groupby(['SEASON', 'CONFERENCE'])\n",
    "#     .apply(lambda df: df.nlargest(1, 'Champion_Prob'))\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# top_by_conference = top_by_conference.sort_values(by='SEASON')\n",
    "\n",
    "# print(top_by_conference[['SEASON', 'CONFERENCE', 'TEAM_NAME', 'Champion_Prob', 'Actual']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc419550-a22b-452c-b32a-ed5ba5410d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF ROC curve\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, proba_rf[:, 1])\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # 4) Plot\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.plot(fpr, tpr, label=f\"RF (AUC = {roc_auc:.3f})\")\n",
    "# plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Guess\")\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"ROC Curve — RF Champion Model\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e46ac-f2e7-448c-b200-2487abb697de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RF feature importance\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 1) Extract importances and pair with feature names\n",
    "# importances = rf.feature_importances_\n",
    "# feat_names  = X_train.columns   # or X_test.columns\n",
    "\n",
    "# imp_series = pd.Series(importances, index=feat_names)\n",
    "\n",
    "# # 2) Sort descending and pick top N (e.g. 10)\n",
    "# top_n = imp_series.sort_values(ascending=False).head(10)\n",
    "\n",
    "# # 3) Plot as a horizontal bar chart\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# top_n.plot.barh()\n",
    "# plt.gca().invert_yaxis()               # largest at top\n",
    "# plt.xlabel(\"Feature Importance\")\n",
    "# plt.title(\"Top 10 Feature Importances — Random Forest\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBA Predict (new)",
   "language": "python",
   "name": "nba-predict-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
